nohup: ignoring input
wandb: Agent Starting Run: oxuig4rr with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.125
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: Currently logged in as: jamesli-wks (jamesli-wks-johns-hopkins-university). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211821-oxuig4rr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/oxuig4rr
Create sweep with ID: 03lyg683
Sweep URL: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<01:14,  2.67it/s] 16%|‚ñà‚ñå        | 31/200 [00:00<00:02, 84.17it/s] 30%|‚ñà‚ñà‚ñà       | 61/200 [00:00<00:00, 144.23it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [00:00<00:00, 187.17it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [00:00<00:00, 218.45it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [00:00<00:00, 238.86it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [00:00<00:00, 255.41it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 189.68it/s]
wandb: - 0.005 MB of 0.015 MB uploadedwandb: \ 0.005 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.29
wandb: best_valid_acc 0.312
wandb:  sub_train_acc 0.28951
wandb: sub_train_loss 0.0619
wandb:       test_acc 0.248
wandb:      valid_acc 0.274
wandb: 
wandb: üöÄ View run vital-sweep-1 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/oxuig4rr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211821-oxuig4rr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 7fhw39jc with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.125
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211837-7fhw39jc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/7fhw39jc
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 287.69it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 291.29it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [00:00<00:00, 293.46it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [00:00<00:00, 293.46it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [00:00<00:00, 293.15it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [00:00<00:00, 293.87it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 293.41it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      valid_acc ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.297
wandb: best_valid_acc 0.322
wandb:  sub_train_acc 0.31425
wandb: sub_train_loss 0.02291
wandb:       test_acc 0.285
wandb:      valid_acc 0.288
wandb: 
wandb: üöÄ View run noble-sweep-2 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/7fhw39jc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211837-7fhw39jc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 179xqrca with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.25
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211852-179xqrca
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/179xqrca
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 288.40it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 292.14it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [00:00<00:00, 290.80it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [00:00<00:00, 292.92it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [00:00<00:00, 293.32it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [00:00<00:00, 292.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 292.02it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.457
wandb: best_valid_acc 0.468
wandb:  sub_train_acc 0.27917
wandb: sub_train_loss 0.00411
wandb:       test_acc 0.271
wandb:      valid_acc 0.306
wandb: 
wandb: üöÄ View run chocolate-sweep-3 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/179xqrca
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211852-179xqrca/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 4sa12mu5 with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.25
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211908-4sa12mu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dashing-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/4sa12mu5
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 296.98it/s] 30%|‚ñà‚ñà‚ñà       | 61/200 [00:00<00:00, 299.00it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 91/200 [00:00<00:00, 294.69it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [00:00<00:00, 295.01it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [00:00<00:00, 297.94it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [00:00<00:00, 299.51it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 298.33it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:      valid_acc ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.482
wandb: best_valid_acc 0.468
wandb:  sub_train_acc 0.35118
wandb: sub_train_loss 0.00671
wandb:       test_acc 0.289
wandb:      valid_acc 0.306
wandb: 
wandb: üöÄ View run dashing-sweep-4 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/4sa12mu5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211908-4sa12mu5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 0iusqk1s with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.375
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211918-0iusqk1s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/0iusqk1s
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 294.02it/s] 30%|‚ñà‚ñà‚ñà       | 60/200 [00:00<00:00, 292.60it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:00<00:00, 293.60it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [00:00<00:00, 296.57it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [00:00<00:00, 298.76it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [00:00<00:00, 300.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 298.19it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:      valid_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.545
wandb: best_valid_acc 0.534
wandb:  sub_train_acc 0.38257
wandb: sub_train_loss 0.00991
wandb:       test_acc 0.316
wandb:      valid_acc 0.306
wandb: 
wandb: üöÄ View run happy-sweep-5 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/0iusqk1s
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211918-0iusqk1s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: mhsulbmi with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.375
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211934-mhsulbmi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/mhsulbmi
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 299.62it/s] 30%|‚ñà‚ñà‚ñà       | 60/200 [00:00<00:00, 299.67it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:00<00:00, 297.40it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [00:00<00:00, 297.69it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [00:00<00:00, 297.51it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [00:00<00:00, 297.39it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 297.63it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:      valid_acc ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.584
wandb: best_valid_acc 0.598
wandb:  sub_train_acc 0.32422
wandb: sub_train_loss 0.0951
wandb:       test_acc 0.285
wandb:      valid_acc 0.3
wandb: 
wandb: üöÄ View run lilac-sweep-6 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/mhsulbmi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211934-mhsulbmi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: g6rt22tq with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.5
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_211949-g6rt22tq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/g6rt22tq
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 288.56it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 291.60it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [00:00<00:00, 293.39it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 120/200 [00:00<00:00, 296.76it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [00:00<00:00, 297.64it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 180/200 [00:00<00:00, 297.17it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 295.62it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.654
wandb: best_valid_acc 0.66
wandb:  sub_train_acc 0.33752
wandb: sub_train_loss 0.06404
wandb:       test_acc 0.435
wandb:      valid_acc 0.44
wandb: 
wandb: üöÄ View run legendary-sweep-7 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/g6rt22tq
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_211949-g6rt22tq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: r59hltsk with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.5
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212003-r59hltsk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/r59hltsk
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 28/200 [00:00<00:00, 278.48it/s] 28%|‚ñà‚ñà‚ñä       | 57/200 [00:00<00:00, 281.63it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 86/200 [00:00<00:00, 281.05it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 115/200 [00:00<00:00, 282.18it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 144/200 [00:00<00:00, 280.83it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 173/200 [00:00<00:00, 280.12it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 280.25it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.651
wandb: best_valid_acc 0.642
wandb:  sub_train_acc 0.26957
wandb: sub_train_loss 0.07908
wandb:       test_acc 0.253
wandb:      valid_acc 0.238
wandb: 
wandb: üöÄ View run devout-sweep-8 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/r59hltsk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212003-r59hltsk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: txzls57h with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.625
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212019-txzls57h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/txzls57h
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 285.52it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 288.91it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [00:00<00:00, 290.46it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [00:00<00:00, 290.91it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [00:00<00:00, 292.06it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 179/200 [00:00<00:00, 293.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 292.17it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÜ
wandb:      valid_acc ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÜ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.666
wandb: best_valid_acc 0.678
wandb:  sub_train_acc 0.411
wandb: sub_train_loss 0.02076
wandb:       test_acc 0.534
wandb:      valid_acc 0.534
wandb: 
wandb: üöÄ View run clear-sweep-9 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/txzls57h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212019-txzls57h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: s6yrih36 with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.625
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212040-s6yrih36
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/s6yrih36
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 290.75it/s] 30%|‚ñà‚ñà‚ñà       | 60/200 [00:00<00:00, 292.20it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:00<00:00, 292.55it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 121/200 [00:00<00:00, 296.42it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 152/200 [00:00<00:00, 298.78it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 183/200 [00:00<00:00, 300.14it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 297.92it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.719
wandb: best_valid_acc 0.71
wandb:  sub_train_acc 0.48227
wandb: sub_train_loss 0.00167
wandb:       test_acc 0.61
wandb:      valid_acc 0.604
wandb: 
wandb: üöÄ View run stellar-sweep-10 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/s6yrih36
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212040-s6yrih36/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: londd0rl with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.25
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212055-londd0rl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/londd0rl
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñé        | 27/200 [00:00<00:00, 269.32it/s] 28%|‚ñà‚ñà‚ñä       | 55/200 [00:00<00:00, 274.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 83/200 [00:00<00:00, 273.00it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 111/200 [00:00<00:00, 268.83it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 139/200 [00:00<00:00, 271.45it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 167/200 [00:00<00:00, 273.02it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 195/200 [00:00<00:00, 273.86it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 272.72it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:      valid_acc ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.427
wandb: best_valid_acc 0.408
wandb:  sub_train_acc 0.34823
wandb: sub_train_loss 0.00781
wandb:       test_acc 0.344
wandb:      valid_acc 0.352
wandb: 
wandb: üöÄ View run earthy-sweep-11 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/londd0rl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212055-londd0rl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: vabjx5ky with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.375
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212111-vabjx5ky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/vabjx5ky
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 285.67it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 289.80it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 88/200 [00:00<00:00, 286.96it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [00:00<00:00, 288.55it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [00:00<00:00, 290.61it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [00:00<00:00, 291.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 289.89it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:      valid_acc ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.56
wandb: best_valid_acc 0.55
wandb:  sub_train_acc 0.29284
wandb: sub_train_loss 0.01407
wandb:       test_acc 0.296
wandb:      valid_acc 0.3
wandb: 
wandb: üöÄ View run faithful-sweep-12 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/vabjx5ky
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212111-vabjx5ky/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: c9u0ar1z with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.75
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212126-c9u0ar1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/c9u0ar1z
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñé        | 27/200 [00:00<00:00, 264.89it/s] 27%|‚ñà‚ñà‚ñã       | 54/200 [00:00<00:00, 266.38it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 82/200 [00:00<00:00, 270.74it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 110/200 [00:00<00:00, 273.21it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 140/200 [00:00<00:00, 282.32it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 170/200 [00:00<00:00, 286.93it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 290.91it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 282.58it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.744
wandb: best_valid_acc 0.752
wandb:  sub_train_acc 0.5938
wandb: sub_train_loss 0.0117
wandb:       test_acc 0.657
wandb:      valid_acc 0.65
wandb: 
wandb: üöÄ View run rosy-sweep-13 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/c9u0ar1z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212126-c9u0ar1z/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 47jwcv5b with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.75
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212141-47jwcv5b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/47jwcv5b
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 284.47it/s] 30%|‚ñà‚ñà‚ñâ       | 59/200 [00:00<00:00, 288.13it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 89/200 [00:00<00:00, 288.97it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 118/200 [00:00<00:00, 288.84it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [00:00<00:00, 289.79it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 178/200 [00:00<00:00, 291.29it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 290.32it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:      valid_acc ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.726
wandb: best_valid_acc 0.712
wandb:  sub_train_acc 0.51551
wandb: sub_train_loss 0.01928
wandb:       test_acc 0.576
wandb:      valid_acc 0.584
wandb: 
wandb: üöÄ View run warm-sweep-14 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/47jwcv5b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212141-47jwcv5b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: j2z526jg with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.875
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212156-j2z526jg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run floral-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/j2z526jg
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 296.04it/s] 30%|‚ñà‚ñà‚ñà       | 60/200 [00:00<00:00, 295.89it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:00<00:00, 286.96it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [00:00<00:00, 282.21it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 148/200 [00:00<00:00, 280.21it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 177/200 [00:00<00:00, 279.76it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 282.45it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.775
wandb: best_valid_acc 0.762
wandb:  sub_train_acc 0.54284
wandb: sub_train_loss 0.38065
wandb:       test_acc 0.57
wandb:      valid_acc 0.558
wandb: 
wandb: üöÄ View run floral-sweep-15 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/j2z526jg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212156-j2z526jg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 7jvgg15b with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.875
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212208-7jvgg15b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/7jvgg15b
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 25/200 [00:00<00:00, 244.80it/s] 25%|‚ñà‚ñà‚ñå       | 50/200 [00:00<00:00, 244.82it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 75/200 [00:00<00:00, 242.32it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 100/200 [00:00<00:00, 242.53it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 125/200 [00:00<00:00, 244.37it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 150/200 [00:00<00:00, 246.11it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 175/200 [00:00<00:00, 247.11it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 247.81it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 245.85it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.775
wandb: best_valid_acc 0.784
wandb:  sub_train_acc 0.67393
wandb: sub_train_loss 0.03985
wandb:       test_acc 0.702
wandb:      valid_acc 0.706
wandb: 
wandb: üöÄ View run polished-sweep-16 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/7jvgg15b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212208-7jvgg15b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: 2xp73fwk with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 1
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212222-2xp73fwk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/2xp73fwk
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 14%|‚ñà‚ñç        | 29/200 [00:00<00:00, 285.63it/s] 29%|‚ñà‚ñà‚ñâ       | 58/200 [00:00<00:00, 287.25it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 87/200 [00:00<00:00, 288.29it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 117/200 [00:00<00:00, 290.60it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [00:00<00:00, 289.09it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [00:00<00:00, 287.51it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 286.87it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.762
wandb: best_valid_acc 0.778
wandb:  sub_train_acc 0.52659
wandb: sub_train_loss 0.30873
wandb:       test_acc 0.508
wandb:      valid_acc 0.514
wandb: 
wandb: üöÄ View run neat-sweep-17 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/2xp73fwk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212222-2xp73fwk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: tlegy36r with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 1
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 1
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212233-tlegy36r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/tlegy36r
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 15%|‚ñà‚ñå        | 30/200 [00:00<00:00, 291.81it/s] 30%|‚ñà‚ñà‚ñà       | 60/200 [00:00<00:00, 291.57it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 90/200 [00:00<00:00, 284.56it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 119/200 [00:00<00:00, 271.30it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 147/200 [00:00<00:00, 270.17it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 176/200 [00:00<00:00, 274.44it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 279.32it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.793
wandb: best_valid_acc 0.788
wandb:  sub_train_acc 0.71381
wandb: sub_train_loss 0.1124
wandb:       test_acc 0.698
wandb:      valid_acc 0.694
wandb: 
wandb: üöÄ View run fresh-sweep-18 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/tlegy36r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212233-tlegy36r/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: qkr2m80n with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.125
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212248-qkr2m80n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/qkr2m80n
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 12%|‚ñà‚ñè        | 24/200 [00:00<00:00, 235.16it/s] 24%|‚ñà‚ñà‚ñç       | 49/200 [00:00<00:00, 239.19it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [00:00<00:00, 240.48it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [00:00<00:00, 239.38it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [00:00<00:00, 240.79it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [00:00<00:00, 240.47it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [00:00<00:00, 241.25it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [00:00<00:00, 241.99it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 240.77it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÖ
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb:      valid_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.28
wandb: best_valid_acc 0.3
wandb:  sub_train_acc 0.2356
wandb: sub_train_loss 0.17204
wandb:       test_acc 0.218
wandb:      valid_acc 0.212
wandb: 
wandb: üöÄ View run classic-sweep-19 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/qkr2m80n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212248-qkr2m80n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: rfm5vjow with config:
wandb: 	activation_type: Relu
wandb: 	batch_size: 1024
wandb: 	dataset_name: CoraGraphDataset
wandb: 	exclusion_type: Largest
wandb: 	hidden_dim: 64
wandb: 	learning_rate: 0.001
wandb: 	model_type: SAGE
wandb: 	num_epochs: 200
wandb: 	num_hidden_layers: 2
wandb: 	optimizer_type: Adam
wandb: 	sample_rate: 0.125
wandb: 	trace_type: Feature
wandb: 	weight_decay: 0.0001
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /home/jamesl/stats_leverage/GNN_test/SAGE_GCN/wandb/run-20240824_212308-rfm5vjow
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: üßπ View sweep at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/sweeps/03lyg683
wandb: üöÄ View run at https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/rfm5vjow
  NumNodes: 2708
  NumEdges: 10556
  NumFeats: 1433
  NumClasses: 7
  NumTrainingSamples: 140
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
num_nodes 2708 num_edges 10556
  0%|          | 0/200 [00:00<?, ?it/s] 12%|‚ñà‚ñè        | 24/200 [00:00<00:00, 235.93it/s] 24%|‚ñà‚ñà‚ñç       | 49/200 [00:00<00:00, 240.71it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 74/200 [00:00<00:00, 241.56it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 99/200 [00:00<00:00, 241.97it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 124/200 [00:00<00:00, 241.81it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 149/200 [00:00<00:00, 240.48it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 174/200 [00:00<00:00, 240.67it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [00:00<00:00, 240.60it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 240.29it/s]
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:  sub_train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá
wandb: sub_train_loss ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       test_acc ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb:      valid_acc ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ
wandb: 
wandb: Run summary:
wandb:  best_test_acc 0.334
wandb: best_valid_acc 0.366
wandb:  sub_train_acc 0.32607
wandb: sub_train_loss 0.18847
wandb:       test_acc 0.198
wandb:      valid_acc 0.208
wandb: 
wandb: üöÄ View run confused-sweep-20 at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp/runs/rfm5vjow
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/jamesli-wks-johns-hopkins-university/SAGE_GCN_hyperparameter_tuning_cora_icassp
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240824_212308-rfm5vjow/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
